"""
?Parsing Natural Scenes and Natural Language with Recursive Neural Networks?
https://ai.stanford.edu/~ang/papers/icml11ParsingWithRecursiveNeuralNetworks.pdf
"""
import torch
import torch.nn as nn
from torch import optim
from torch.nn.utils import clip_grad_norm_
import word_vector as wv
import numpy as np
import time
import random
import dgl


class RvNNCell(nn.Module):
    def __init__(self, emb_dim):
        super(RvNNCell, self).__init__()
        self.W = nn.Linear(2*emb_dim, emb_dim)

    # The source nodes send messages(source node features) through edges to the destinations.
    def message_func(self, edges):
        return {'p': edges.src['p']}
    
    # The destination nodes need to receive the messages and potentially update node features.
    def reduce_func(self, nodes):
        p_cat = nodes.mailbox['p'].view(nodes.mailbox['p'].size(0), -1)   
        p = torch.tanh(self.W(p_cat))
        return {'p': p}


class RvNN(nn.Module):
    def __init__(self, word_dict, vocab_size, emb_dim, class_size, drop_prob):
        super(RvNN, self).__init__()
        self.dict = word_dict  
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.embedding = self.load_pretrained()
        self.dropout = nn.Dropout(drop_prob)
        self.W_label = nn.Linear(emb_dim, class_size)
        self.cell = RvNNCell(emb_dim)

    def load_pretrained(self):
        w2v = wv.load_w2v("./sst-Glove-vectors.txt")
        # w2v = wv.load_w2v("./simplify_w2v_all.txt", state=False)  # Google_300

        W = np.zeros((self.vocab_size, self.emb_dim))

        count = 0
        for i, w in enumerate(self.dict):
            if w in w2v.vocab:
                W[i, :] = w2v[w]
                count += 1
            else:
                # print("no word:{}\n".format(w))
                W[i, :] = np.random.uniform(-0.0001, 0.0001, self.emb_dim)

        emb_mat = nn.Embedding.from_pretrained(torch.tensor(W, dtype=torch.float), freeze=False)  # embed can be learned
        print("{} out of {} vectors loaded!".format(count, self.vocab_size))

        del W
        del w2v

        return emb_mat


    def forward(self, batch):
        g = batch.graph
        # Set the function defined to be the default message function
        g.register_message_func(self.cell.message_func)
        g.register_reduce_func(self.cell.reduce_func)
        
        embeds = self.embedding(batch.wordid * batch.mask)

        g.ndata['p'] = self.dropout(embeds)  # add dropout --> improve BestRoot_acc_test
        # Define traversal:trigger the message passing
        dgl.prop_nodes_topo(g)  # Message propagation using node frontiers generated by topological order. 
        p = self.dropout(g.ndata.pop('p')) # pop():get and remove node states from the graph-->save memory
        # compute logits
        logits = self.W_label(p)
        return logits




